{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/9645258/aiffel/blob/main/02_goingdeeper/01_nlp/%5BGD-16%5Dnlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GD NLP 02 : HuggingFace 커스텀 프로젝트 만들기**"
      ],
      "metadata": {
        "id": "kNJJ07A0O82g"
      },
      "id": "kNJJ07A0O82g"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Train/Evaluation과 Test**"
      ],
      "metadata": {
        "id": "DqtPYJXFO2iy"
      },
      "id": "DqtPYJXFO2iy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **MNLI 데이터셋을 처리하는 전용 Processor 클래스를 정상적으로 구현하였다.**  \n",
        "Processor 클래스에 대해 1개 이상의 example에 대한 단위테스트가 정상 진행되었다.  \n",
        "\n",
        "- **BERT tokenizer와 Processor를 결합하여 데이터셋을 정상적으로 생성하였다.**  \n",
        "MNLI 데이터셋의 입력과 라벨의 정의에 잘 맞는 tf.data.Dataset 인스턴스가 얻어졌다.  \n",
        "\n",
        "- **MNLI 데이터셋에 대해 적당한 모델을 fine-tuning하여 학습하였다.**  \n",
        "모델 학습이 정상적으로 진행되었다.\n"
      ],
      "metadata": {
        "id": "MUyNVn4iOnzm"
      },
      "id": "MUyNVn4iOnzm"
    },
    {
      "cell_type": "markdown",
      "id": "e0acd575",
      "metadata": {
        "id": "e0acd575"
      },
      "source": [
        "## **1. 데이터 준비**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "151e9a69",
      "metadata": {
        "id": "151e9a69"
      },
      "source": [
        "### **1-1. 라이브러리 import**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ed7aa19",
      "metadata": {
        "id": "7ed7aa19"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dataclasses import asdict\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action = 'ignore')\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import transformers\n",
        "transformers.logging.set_verbosity_error()\n",
        "\n",
        "from transformers.data.processors.utils import DataProcessor, InputExample, InputFeatures\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from transformers import RobertaTokenizer, TFRobertaForSequenceClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b01de17d",
      "metadata": {
        "id": "b01de17d"
      },
      "source": [
        "### **1-2. 데이터 load**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35e5080d",
      "metadata": {
        "id": "35e5080d"
      },
      "outputs": [],
      "source": [
        "dataset, info = tfds.load('glue/mnli', with_info=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc666665",
      "metadata": {
        "id": "dc666665"
      },
      "source": [
        "### **1-3. 데이터 확인**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38d87f73",
      "metadata": {
        "id": "38d87f73"
      },
      "source": [
        "- **데이터 분류 및 각 데이터 분류에 포함된 데이터 수 확인**  \n",
        "train / validation_matched / validation_mismatched / test_matched / test_mismatched 총 5개의 데이터 분류  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76c80b89",
      "metadata": {
        "id": "76c80b89",
        "outputId": "580f7c4c-1d25-4865-fbb6-649364648ea5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0: train(392,702)\n",
            "1: validation_matched(9,815)\n",
            "2: validation_mismatched(9,832)\n",
            "3: test_matched(9,796)\n",
            "4: test_mismatched(9,847)\n"
          ]
        }
      ],
      "source": [
        "for idx, (domain, data) in enumerate(dataset.items()):\n",
        "    print(f\"{idx}: {domain}({len(data):,})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c2d2b39",
      "metadata": {
        "id": "7c2d2b39"
      },
      "source": [
        "- **train 데이터 확인**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27754a6a",
      "metadata": {
        "id": "27754a6a",
        "outputId": "2b70520d-82ed-4b1d-8ed7-e0e612426531"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: {hypothesis: (), idx: (), label: (), premise: ()}, types: {hypothesis: tf.string, idx: tf.int32, label: tf.int64, premise: tf.string}>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset['train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85de835a",
      "metadata": {
        "id": "85de835a",
        "outputId": "da84eb73-84d6-473f-bb87-fad3be33b849"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hypothesis: b'Meaningful partnerships with stakeholders is crucial.'\n",
            "idx: 16399\n",
            "Premise: b'In recognition of these tensions, LSC has worked diligently since 1995 to convey the expectations of the State Planning Initiative and to establish meaningful partnerships with stakeholders aimed at fostering a new symbiosis between the federal provider and recipients of legal services funding.'\n",
            "Label: 1\n"
          ]
        }
      ],
      "source": [
        "for data in dataset['train']:\n",
        "    print(\"Hypothesis:\", data['hypothesis'].numpy(), end=\"\\n\")\n",
        "    print(\"idx:\", data['idx'].numpy(), end=\"\\n\")    \n",
        "    print(\"Premise:\", data['premise'].numpy(), end=\"\\n\")\n",
        "    print(\"Label:\", data['label'].numpy())    \n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e1aa9da",
      "metadata": {
        "id": "6e1aa9da"
      },
      "source": [
        "- **validation_matched 데이터 확인**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d789b1c5",
      "metadata": {
        "id": "d789b1c5",
        "outputId": "5465e2ea-ffdd-48f6-d05b-8772890e9bc6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: {hypothesis: (), idx: (), label: (), premise: ()}, types: {hypothesis: tf.string, idx: tf.int32, label: tf.int64, premise: tf.string}>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset['validation_matched']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3029ed69",
      "metadata": {
        "id": "3029ed69",
        "outputId": "b983e8e9-6563-4db4-d574-59daaa4acd44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hypothesis: b'yeah lots of people for the right life '\n",
            "idx: 6287\n",
            "Premise: b'uh-huh oh yeah all the people for right uh life or something'\n",
            "Label: 0\n"
          ]
        }
      ],
      "source": [
        "for data in dataset['validation_matched']:\n",
        "    print(\"Hypothesis:\", data['hypothesis'].numpy(), end=\"\\n\")\n",
        "    print(\"idx:\", data['idx'].numpy(), end=\"\\n\")    \n",
        "    print(\"Premise:\", data['premise'].numpy(), end=\"\\n\")\n",
        "    print(\"Label:\", data['label'].numpy())\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c3ce84b",
      "metadata": {
        "id": "5c3ce84b"
      },
      "source": [
        "- **validation_mismatched 데이터 확인**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e07f5019",
      "metadata": {
        "id": "e07f5019",
        "outputId": "dc915ee1-c96e-455f-eb51-d7087ecda77a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: {hypothesis: (), idx: (), label: (), premise: ()}, types: {hypothesis: tf.string, idx: tf.int32, label: tf.int64, premise: tf.string}>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset['validation_mismatched']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9c77bca",
      "metadata": {
        "id": "a9c77bca",
        "outputId": "5c987b37-2c35-4d1b-fea8-bda235ce4f8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hypothesis: b\"These projects are largely ignored and don't impact anyone. \"\n",
            "idx: 9410\n",
            "Premise: b'Projects which enliven and enrich the student experience and draw some of our finest scholars and teachers to our campus--and to our city.'\n",
            "Label: 2\n"
          ]
        }
      ],
      "source": [
        "for data in dataset['validation_mismatched']:\n",
        "    print(\"Hypothesis:\", data['hypothesis'].numpy(), end=\"\\n\")\n",
        "    print(\"idx:\", data['idx'].numpy(), end=\"\\n\")    \n",
        "    print(\"Premise:\", data['premise'].numpy(), end=\"\\n\")\n",
        "    print(\"Label:\", data['label'].numpy())\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a79f4ca3",
      "metadata": {
        "id": "a79f4ca3"
      },
      "source": [
        "- **test_matched 데이터 확인**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dfd65cb",
      "metadata": {
        "id": "4dfd65cb",
        "outputId": "1d6ac5bf-c432-4f94-869c-7b30b8e2397b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: {hypothesis: (), idx: (), label: (), premise: ()}, types: {hypothesis: tf.string, idx: tf.int32, label: tf.int64, premise: tf.string}>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset['test_matched']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fde98d6",
      "metadata": {
        "id": "1fde98d6",
        "outputId": "939e0769-17ad-43a9-e3fd-496e2ce0db75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hypothesis: b'Is there a pump that you put in there?'\n",
            "idx: 5398\n",
            "Premise: b'well is there a little electric pump you put in there'\n",
            "Label: -1\n"
          ]
        }
      ],
      "source": [
        "for data in dataset['test_matched']:\n",
        "    print(\"Hypothesis:\", data['hypothesis'].numpy(), end=\"\\n\")\n",
        "    print(\"idx:\", data['idx'].numpy(), end=\"\\n\")    \n",
        "    print(\"Premise:\", data['premise'].numpy(), end=\"\\n\")\n",
        "    print(\"Label:\", data['label'].numpy())\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0edd04a2",
      "metadata": {
        "id": "0edd04a2"
      },
      "source": [
        "- **test_mismatched 데이터 확인**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c5a35fe",
      "metadata": {
        "id": "2c5a35fe",
        "outputId": "6125ba78-8822-4fc9-a7b3-51a5d8033ab0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: {hypothesis: (), idx: (), label: (), premise: ()}, types: {hypothesis: tf.string, idx: tf.int32, label: tf.int64, premise: tf.string}>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset['test_mismatched']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89f63a54",
      "metadata": {
        "id": "89f63a54",
        "outputId": "a8881070-313f-4d94-d4e7-ad833fcf545c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hypothesis: b'That table has been in my family for generations.'\n",
            "idx: 3498\n",
            "Premise: b'Anyway, I treasure that table.'\n",
            "Label: -1\n"
          ]
        }
      ],
      "source": [
        "for data in dataset['test_mismatched']:\n",
        "    print(\"Hypothesis:\", data['hypothesis'].numpy(), end=\"\\n\")\n",
        "    print(\"idx:\", data['idx'].numpy(), end=\"\\n\")    \n",
        "    print(\"Premise:\", data['premise'].numpy(), end=\"\\n\")\n",
        "    print(\"Label:\", data['label'].numpy())\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a889f8f9",
      "metadata": {
        "id": "a889f8f9"
      },
      "source": [
        "## **2. 데이터 전처리**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc5a3982",
      "metadata": {
        "id": "cc5a3982"
      },
      "source": [
        "### **2-1. 데이터셋 split**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc7248f7",
      "metadata": {
        "id": "fc7248f7"
      },
      "source": [
        "- **데이터셋 split 함수 정의**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5c19611",
      "metadata": {
        "id": "b5c19611"
      },
      "outputs": [],
      "source": [
        "def split_dataset(dataset, val_size):    \n",
        "    dataset = dataset.shuffle(len(dataset))\n",
        "    val_dataset = dataset.shuffle(len(dataset)).take(val_size)\n",
        "    test_dataset = dataset.shuffle(len(dataset)).skip(val_size)\n",
        "    return (val_dataset, test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b42fa869",
      "metadata": {
        "id": "b42fa869"
      },
      "source": [
        "- **data 데이터셋 결합**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5c079d8",
      "metadata": {
        "id": "e5c079d8"
      },
      "outputs": [],
      "source": [
        "matched_data = dataset['test_matched'].concatenate(dataset['validation_matched'])\n",
        "mismatched_data = dataset['test_mismatched'].concatenate(dataset['validation_mismatched'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebf21d40",
      "metadata": {
        "id": "ebf21d40"
      },
      "source": [
        "- **데이터셋 split 진행**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "456b98b1",
      "metadata": {
        "id": "456b98b1"
      },
      "outputs": [],
      "source": [
        "matched_val, matched_test = split_dataset(matched_data, len(dataset['validation_matched']))\n",
        "mismatched_val, mismatched_test = split_dataset(mismatched_data, len(dataset['validation_mismatched']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49386fa9",
      "metadata": {
        "id": "49386fa9"
      },
      "source": [
        "- **val 데이터셋 결합**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1fa52c2",
      "metadata": {
        "id": "c1fa52c2"
      },
      "outputs": [],
      "source": [
        "val = mismatched_val.concatenate(matched_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39bf8e0c",
      "metadata": {
        "id": "39bf8e0c"
      },
      "source": [
        "- **데이터 수 확인**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a477e2a4",
      "metadata": {
        "id": "a477e2a4",
        "outputId": "e0b49518-e806-4c56-89c1-9152f330e14d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "학습 데이터에 포함된 validation 데이터 수: 9815\n",
            "학습 데이터에 포함되지 않은 validation 데이터 수: 9832\n",
            "학습 데이터에 포함된 test 데이터 수: 9796\n",
            "학습 데이터에 포함되지 않은 test 데이터 수: 9847\n"
          ]
        }
      ],
      "source": [
        "print('학습 데이터에 포함된 validation 데이터 수:', len(matched_val))\n",
        "print('학습 데이터에 포함되지 않은 validation 데이터 수:', len(mismatched_val))\n",
        "print('학습 데이터에 포함된 test 데이터 수:', len(matched_test))\n",
        "print('학습 데이터에 포함되지 않은 test 데이터 수:', len(mismatched_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08636fbc",
      "metadata": {
        "id": "08636fbc"
      },
      "source": [
        "### **2-2. MNLI Processor**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f5c3760",
      "metadata": {
        "id": "0f5c3760"
      },
      "source": [
        "- **추상 클래스 생성**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29a6d2f5",
      "metadata": {
        "id": "29a6d2f5"
      },
      "outputs": [],
      "source": [
        "class DataProcessor:\n",
        "\n",
        "    def get_example_from_tensor_dict(self, tensor_dict):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_test_examples(self, data_dir):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_labels(self):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def tfds_map(self, example):\n",
        "        if len(self.get_labels()) > 1:\n",
        "            example.label = self.get_labels()[int(example.label)]\n",
        "        return example\n",
        "\n",
        "    @classmethod\n",
        "    def _read_tsv(cls, input_file, quotechar=None):\n",
        "        with open(input_file, \"r\", encoding=\"utf-8-sig\") as f:\n",
        "            return list(csv.reader(f, delimiter=\"\\t\", quotechar=quotechar))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00545eda",
      "metadata": {
        "id": "00545eda"
      },
      "source": [
        "- **추상 클래스 상속**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c0fea28",
      "metadata": {
        "id": "6c0fea28"
      },
      "outputs": [],
      "source": [
        "class MnliProcessor(DataProcessor):\n",
        "    \n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def get_example_from_tensor_dict(self, tensor_dict):\n",
        "        return InputExample(tensor_dict[\"idx\"].numpy(), tensor_dict[\"premise\"].numpy().decode(\"utf-8\"), \n",
        "                            tensor_dict[\"hypothesis\"].numpy().decode(\"utf-8\"), str(tensor_dict[\"label\"].numpy()), )\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        print(\"LOOKING AT {}\".format(os.path.join(data_dir, \"train.tsv\")))\n",
        "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"dev_matched.tsv\")), \"dev_matched\")\n",
        "\n",
        "    def get_test_examples(self, data_dir):\n",
        "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"test_matched.tsv\")), \"test_matched\")\n",
        "\n",
        "    def get_labels(self):\n",
        "        return [\"contradiction\", \"entailment\", \"neutral\"]\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "            if i == 0:\n",
        "                continue\n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            text_a = line[3]\n",
        "            text_b = line[4]\n",
        "            label = None if set_type == \"test\" else line[0]\n",
        "            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
        "        return examples"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f172d32",
      "metadata": {
        "id": "9f172d32"
      },
      "source": [
        "- **MNLI Processor을 이용한 전처리 예시 확인**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "513cbd3b",
      "metadata": {
        "id": "513cbd3b"
      },
      "outputs": [],
      "source": [
        "processor = MnliProcessor()\n",
        "examples = dataset['train'].take(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30abd0c2",
      "metadata": {
        "scrolled": true,
        "id": "30abd0c2",
        "outputId": "daa6b5ef-c61c-4d2a-a339-1aabf2524296"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<원본 데이터>\n",
            "<class 'dict'>\n",
            "{'hypothesis': <tf.Tensor: shape=(), dtype=string, numpy=b'Meaningful partnerships with stakeholders is crucial.'>, 'idx': <tf.Tensor: shape=(), dtype=int32, numpy=16399>, 'label': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'premise': <tf.Tensor: shape=(), dtype=string, numpy=b'In recognition of these tensions, LSC has worked diligently since 1995 to convey the expectations of the State Planning Initiative and to establish meaningful partnerships with stakeholders aimed at fostering a new symbiosis between the federal provider and recipients of legal services funding.'>} \n",
            "\n",
            "\n",
            "<Processed 데이터>\n",
            "<class 'transformers.data.processors.utils.InputExample'>\n",
            "InputExample(guid=16399, text_a='In recognition of these tensions, LSC has worked diligently since 1995 to convey the expectations of the State Planning Initiative and to establish meaningful partnerships with stakeholders aimed at fostering a new symbiosis between the federal provider and recipients of legal services funding.', text_b='Meaningful partnerships with stakeholders is crucial.', label='1') \n",
            "\n",
            "\n",
            "<라벨>\n",
            "{'contradiction': 0, 'entailment': 1, 'neutral': 2}\n"
          ]
        }
      ],
      "source": [
        "for example in examples:\n",
        "    \n",
        "    print(\"<원본 데이터>\", type(example), sep = \"\\n\")\n",
        "    print(example, \"\\n\\n\")\n",
        "    \n",
        "    example = processor.get_example_from_tensor_dict(example)\n",
        "    \n",
        "    print(\"<Processed 데이터>\", type(example), sep = \"\\n\")\n",
        "    print(example, \"\\n\\n\")\n",
        "\n",
        "    label_map = {label: i for i, label in enumerate(processor.get_labels())}\n",
        "    print(\"<라벨>\")\n",
        "    print(label_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d484511",
      "metadata": {
        "id": "3d484511"
      },
      "source": [
        "## **3. 데이터셋 구성**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77b2cd41",
      "metadata": {
        "id": "77b2cd41"
      },
      "source": [
        "### **3-1. 정수 인코딩**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8901fed",
      "metadata": {
        "id": "b8901fed"
      },
      "source": [
        "- **데이터 정수화 함수 정의**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb7881ce",
      "metadata": {
        "id": "bb7881ce"
      },
      "outputs": [],
      "source": [
        "def _glue_convert_examples_to_features(examples, tokenizer, max_length, processor, label_list=None, output_mode=\"claasification\") :\n",
        "    \n",
        "    if max_length is None :\n",
        "        max_length = tokenizer.max_len\n",
        "        \n",
        "    if label_list is None:\n",
        "        label_list = processor.get_labels()\n",
        "\n",
        "    label_map = {label: i for i, label in enumerate(label_list)}\n",
        "    labels = [label_map[example.label] for example in examples]\n",
        "\n",
        "    batch_encoding = tokenizer([(example.text_a, example.text_b) for example in examples], \n",
        "                               max_length=max_length, padding=\"max_length\", truncation=True, )\n",
        "\n",
        "    features = []\n",
        "    \n",
        "    for i in range(len(examples)):\n",
        "        inputs = {k: batch_encoding[k][i] for k in batch_encoding}\n",
        "\n",
        "        feature = InputFeatures(**inputs, label=labels[i])\n",
        "        features.append(feature)\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a2561fc",
      "metadata": {
        "id": "2a2561fc"
      },
      "source": [
        "### **3-2. 데이터셋 생성**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f87ef5fb",
      "metadata": {
        "id": "f87ef5fb"
      },
      "source": [
        "- **정수화 진행 데이터 데이터셋화 함수 정의**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a089e8f6",
      "metadata": {
        "id": "a089e8f6"
      },
      "outputs": [],
      "source": [
        "def tf_glue_convert_examples_to_features(examples, tokenizer, max_length, processor, label_list=None, output_mode=\"classification\") :\n",
        "\n",
        "    examples = [processor.tfds_map(processor.get_example_from_tensor_dict(example)) for example in examples]\n",
        "    features = _glue_convert_examples_to_features(examples, tokenizer, max_length, processor)\n",
        "    label_type = tf.int64\n",
        "\n",
        "    def gen():\n",
        "        for ex in features:\n",
        "            d = {k: v for k, v in asdict(ex).items() if v is not None}\n",
        "            label = d.pop(\"label\")\n",
        "            yield (d, label)\n",
        "\n",
        "    input_names = [\"input_ids\"] + tokenizer.model_input_names\n",
        "\n",
        "    return tf.data.Dataset.from_generator(gen, ({k: tf.int32 for k in input_names}, label_type),\n",
        "                                          ({k: tf.TensorShape([None]) for k in input_names}, tf.TensorShape([])), )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5f9c074",
      "metadata": {
        "id": "f5f9c074"
      },
      "source": [
        "### **3-3. 토크나이저**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f98cc0b",
      "metadata": {
        "id": "5f98cc0b"
      },
      "source": [
        "- **토크나이저 import**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dd726b2",
      "metadata": {
        "id": "8dd726b2"
      },
      "outputs": [],
      "source": [
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", use_fast=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aee63bf0",
      "metadata": {
        "id": "aee63bf0"
      },
      "source": [
        "### **3-4. 데이터셋 구성**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec9f7813",
      "metadata": {
        "id": "ec9f7813"
      },
      "source": [
        "- **학습 데이터셋 생성**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed1c5695",
      "metadata": {
        "scrolled": false,
        "id": "ed1c5695"
      },
      "outputs": [],
      "source": [
        "def get_dataset_batch(raw_dataset_list, tokenizer, processor):\n",
        "    \n",
        "    dataset_list = []\n",
        "    \n",
        "    for idx, dataset in enumerate(raw_dataset_list):\n",
        "        data = tf_glue_convert_examples_to_features(dataset, tokenizer, max_length=128, processor=processor)\n",
        "        \n",
        "        if idx == 0:\n",
        "            data_batch = data.shuffle(100).batch(16).repeat(2)\n",
        "        else:\n",
        "            data_batch = data.shuffle(100).batch(16)\n",
        "        dataset_list.append(data_batch)\n",
        "    return dataset_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b56b13d4",
      "metadata": {
        "id": "b56b13d4"
      },
      "outputs": [],
      "source": [
        "raw_dataset_list = (dataset['train'], val, matched_test, mismatched_test)\n",
        "bert_train, bert_val, bert_m_test, bert_mism_test = get_dataset_batch(raw_dataset_list, bert_tokenizer, processor)\n",
        "robert_train, robert_val, robert_m_test, robert_mism_test = get_dataset_batch(raw_dataset_list, roberta_tokenizer, processor)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "831d207c",
      "metadata": {
        "id": "831d207c"
      },
      "source": [
        "## **4. 모델 생성 및 학습 진행**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d353618",
      "metadata": {
        "id": "2d353618"
      },
      "source": [
        "### **4-1. 옵티마이저**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fcf2b8e",
      "metadata": {
        "id": "9fcf2b8e"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb94e664",
      "metadata": {
        "id": "fb94e664"
      },
      "source": [
        "### **4-2. BERT**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4515903e",
      "metadata": {
        "id": "4515903e"
      },
      "source": [
        "- **모델 생성**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddb0ad7d",
      "metadata": {
        "scrolled": false,
        "id": "ddb0ad7d",
        "outputId": "6296c03f-0be1-4115-85d1-03364b68ec45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"tf_bert_for_sequence_classification\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bert (TFBertMainLayer)       multiple                  109482240 \n",
            "_________________________________________________________________\n",
            "dropout_37 (Dropout)         multiple                  0         \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           multiple                  2307      \n",
            "=================================================================\n",
            "Total params: 109,484,547\n",
            "Trainable params: 109,484,547\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "bert_model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
        "bert_model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
        "bert_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0cedfcc",
      "metadata": {
        "id": "d0cedfcc"
      },
      "source": [
        "- **모델 학습**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9417882d",
      "metadata": {
        "scrolled": false,
        "id": "9417882d",
        "outputId": "b4e6c712-5844-4836-ac5d-9ab08de5ee01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "500/500 [==============================] - 119s 217ms/step - loss: 0.8550 - acc: 0.6111 - val_loss: 1.0582 - val_acc: 0.5197\n",
            "Epoch 2/20\n",
            "500/500 [==============================] - 108s 217ms/step - loss: 0.7099 - acc: 0.7056 - val_loss: 1.2419 - val_acc: 0.5454\n",
            "Epoch 3/20\n",
            "500/500 [==============================] - 108s 216ms/step - loss: 0.6380 - acc: 0.7425 - val_loss: 1.1351 - val_acc: 0.5632\n",
            "Epoch 4/20\n",
            "500/500 [==============================] - 106s 213ms/step - loss: 0.6089 - acc: 0.7505 - val_loss: 1.2829 - val_acc: 0.5590\n",
            "Epoch 5/20\n",
            "500/500 [==============================] - 107s 214ms/step - loss: 0.6086 - acc: 0.7571 - val_loss: 1.1551 - val_acc: 0.5468\n",
            "Epoch 6/20\n",
            "500/500 [==============================] - 107s 214ms/step - loss: 0.5802 - acc: 0.7691 - val_loss: 1.4161 - val_acc: 0.5321\n",
            "Epoch 7/20\n",
            "500/500 [==============================] - 107s 215ms/step - loss: 0.5744 - acc: 0.7673 - val_loss: 1.2186 - val_acc: 0.5653\n",
            "Epoch 8/20\n",
            "500/500 [==============================] - 108s 215ms/step - loss: 0.5696 - acc: 0.7709 - val_loss: 1.4352 - val_acc: 0.5640\n",
            "Epoch 9/20\n",
            "500/500 [==============================] - 108s 216ms/step - loss: 0.5440 - acc: 0.7816 - val_loss: 1.1862 - val_acc: 0.5838\n",
            "Epoch 10/20\n",
            "500/500 [==============================] - 108s 217ms/step - loss: 0.5565 - acc: 0.7795 - val_loss: 1.4045 - val_acc: 0.5739\n",
            "Epoch 11/20\n",
            "500/500 [==============================] - 108s 216ms/step - loss: 0.5390 - acc: 0.7837 - val_loss: 1.3035 - val_acc: 0.5512\n",
            "Epoch 12/20\n",
            "500/500 [==============================] - 108s 216ms/step - loss: 0.5392 - acc: 0.7836 - val_loss: 1.2050 - val_acc: 0.5725\n",
            "Epoch 13/20\n",
            "500/500 [==============================] - 119s 238ms/step - loss: 0.5429 - acc: 0.7826 - val_loss: 1.4264 - val_acc: 0.5500\n",
            "Epoch 14/20\n",
            "500/500 [==============================] - 107s 215ms/step - loss: 0.5274 - acc: 0.7909 - val_loss: 1.3284 - val_acc: 0.5793\n",
            "Epoch 15/20\n",
            "500/500 [==============================] - 108s 216ms/step - loss: 0.5400 - acc: 0.7883 - val_loss: 1.2365 - val_acc: 0.5826\n",
            "Epoch 16/20\n",
            "500/500 [==============================] - 108s 216ms/step - loss: 0.5351 - acc: 0.7894 - val_loss: 1.2792 - val_acc: 0.5755\n",
            "Epoch 17/20\n",
            "500/500 [==============================] - 108s 217ms/step - loss: 0.5322 - acc: 0.7934 - val_loss: 1.5491 - val_acc: 0.5324\n",
            "Epoch 18/20\n",
            "500/500 [==============================] - 107s 215ms/step - loss: 0.5130 - acc: 0.7993 - val_loss: 1.2834 - val_acc: 0.5817\n",
            "Epoch 19/20\n",
            "500/500 [==============================] - 108s 216ms/step - loss: 0.5161 - acc: 0.7921 - val_loss: 1.3910 - val_acc: 0.5786\n",
            "Epoch 20/20\n",
            "500/500 [==============================] - 108s 216ms/step - loss: 0.5314 - acc: 0.7840 - val_loss: 1.1041 - val_acc: 0.6016\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x229ca038b50>"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bert_model.fit(bert_train, epochs=20, steps_per_epoch=500, validation_data=bert_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "099786ca",
      "metadata": {
        "id": "099786ca"
      },
      "source": [
        "### **4-3. RoBERTa**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccccae56",
      "metadata": {
        "id": "ccccae56"
      },
      "source": [
        "- **모델 생성**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0353c6d3",
      "metadata": {
        "id": "0353c6d3",
        "outputId": "9bb80a77-b714-4701-e200-7a07fbe7d17d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"tf_roberta_for_sequence_classification\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "roberta (TFRobertaMainLayer) multiple                  124055040 \n",
            "_________________________________________________________________\n",
            "classifier (TFRobertaClassif multiple                  592899    \n",
            "=================================================================\n",
            "Total params: 124,647,939\n",
            "Trainable params: 124,647,939\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "roberta_model = TFRobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=3)\n",
        "roberta_model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
        "roberta_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0d3d6e8",
      "metadata": {
        "id": "a0d3d6e8"
      },
      "source": [
        "- **모델 학습**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "720ff7a7",
      "metadata": {
        "id": "720ff7a7",
        "outputId": "989cee5d-b0bb-4ab1-8792-7ae3ded6ca5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "500/500 [==============================] - 115s 212ms/step - loss: 1.1049 - acc: 0.3246 - val_loss: 1.1445 - val_acc: 0.1767\n",
            "Epoch 2/20\n",
            "500/500 [==============================] - 104s 209ms/step - loss: 1.1007 - acc: 0.3302 - val_loss: 1.1129 - val_acc: 0.1767\n",
            "Epoch 3/20\n",
            "500/500 [==============================] - 105s 210ms/step - loss: 1.1010 - acc: 0.3306 - val_loss: 1.1171 - val_acc: 0.1767\n",
            "Epoch 4/20\n",
            "500/500 [==============================] - 105s 209ms/step - loss: 1.1003 - acc: 0.3391 - val_loss: 1.1339 - val_acc: 0.1589\n",
            "Epoch 5/20\n",
            "500/500 [==============================] - 105s 211ms/step - loss: 1.1001 - acc: 0.3269 - val_loss: 1.0862 - val_acc: 0.6645\n",
            "Epoch 6/20\n",
            "500/500 [==============================] - 105s 211ms/step - loss: 1.0997 - acc: 0.3414 - val_loss: 1.0739 - val_acc: 0.6645\n",
            "Epoch 7/20\n",
            "500/500 [==============================] - 105s 211ms/step - loss: 1.1004 - acc: 0.3298 - val_loss: 1.0914 - val_acc: 0.1767\n",
            "Epoch 8/20\n",
            "500/500 [==============================] - 105s 211ms/step - loss: 1.0997 - acc: 0.3391 - val_loss: 1.1216 - val_acc: 0.1767\n",
            "Epoch 9/20\n",
            "500/500 [==============================] - 105s 211ms/step - loss: 1.0999 - acc: 0.3254 - val_loss: 1.0843 - val_acc: 0.6645\n",
            "Epoch 10/20\n",
            "500/500 [==============================] - 105s 210ms/step - loss: 1.1004 - acc: 0.3329 - val_loss: 1.1095 - val_acc: 0.1767\n",
            "Epoch 11/20\n",
            "500/500 [==============================] - 106s 211ms/step - loss: 1.0999 - acc: 0.3300 - val_loss: 1.1155 - val_acc: 0.1767\n",
            "Epoch 12/20\n",
            "500/500 [==============================] - 106s 213ms/step - loss: 1.0997 - acc: 0.3365 - val_loss: 1.0877 - val_acc: 0.6645\n",
            "Epoch 13/20\n",
            "500/500 [==============================] - 107s 214ms/step - loss: 1.1005 - acc: 0.3186 - val_loss: 1.0877 - val_acc: 0.6645\n",
            "Epoch 14/20\n",
            "500/500 [==============================] - 105s 209ms/step - loss: 1.0996 - acc: 0.3339 - val_loss: 1.1340 - val_acc: 0.1589\n",
            "Epoch 15/20\n",
            "500/500 [==============================] - 105s 209ms/step - loss: 1.1002 - acc: 0.3456 - val_loss: 1.0815 - val_acc: 0.6645\n",
            "Epoch 16/20\n",
            "500/500 [==============================] - 105s 211ms/step - loss: 1.0993 - acc: 0.3451 - val_loss: 1.1071 - val_acc: 0.1767\n",
            "Epoch 17/20\n",
            "500/500 [==============================] - 105s 210ms/step - loss: 1.0999 - acc: 0.3360 - val_loss: 1.1182 - val_acc: 0.1589\n",
            "Epoch 18/20\n",
            "500/500 [==============================] - 103s 207ms/step - loss: 1.1006 - acc: 0.3262 - val_loss: 1.0860 - val_acc: 0.6645\n",
            "Epoch 19/20\n",
            "500/500 [==============================] - 104s 208ms/step - loss: 1.1002 - acc: 0.3291 - val_loss: 1.0972 - val_acc: 0.6645\n",
            "Epoch 20/20\n",
            "500/500 [==============================] - 104s 208ms/step - loss: 1.0998 - acc: 0.3386 - val_loss: 1.0624 - val_acc: 0.6645\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x229f1a23220>"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "roberta_model.fit(robert_train, epochs=20, steps_per_epoch=500, validation_data=robert_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddac648f",
      "metadata": {
        "id": "ddac648f"
      },
      "source": [
        "## **5. 모델 평가**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fca0cef",
      "metadata": {
        "id": "3fca0cef"
      },
      "source": [
        "### **5-1. BERT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d519628c",
      "metadata": {
        "id": "d519628c",
        "outputId": "6a86c1a0-8c0a-4ac0-eb6a-69a49a89c925"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "613/613 [==============================] - 26s 43ms/step - loss: 1.1171 - acc: 0.5991\n",
            "616/616 [==============================] - 27s 43ms/step - loss: 1.1076 - acc: 0.5962\n"
          ]
        }
      ],
      "source": [
        "bert_matched_result = bert_model.evaluate(bert_m_test)\n",
        "bert_mismatched_result = bert_model.evaluate(bert_mism_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f46d7397",
      "metadata": {
        "id": "f46d7397",
        "outputId": "bdf8ccc6-b1e0-478d-cf28-7a0827ce78ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<학습 데이터에 포함된 test 데이터>\n",
            "Loss: 1.117\n",
            "Accuracy: 0.599\n",
            "\n",
            "\n",
            "<학습 데이터에 포함되지 않은 test 데이터>\n",
            "Loss: 1.108\n",
            "Accuracy: 0.596\n"
          ]
        }
      ],
      "source": [
        "print(f\"<학습 데이터에 포함된 test 데이터>\\nLoss: {bert_matched_result[0]:.3f}\\nAccuracy: {bert_matched_result[1]:.3f}\\n\\n\")\n",
        "print(f\"<학습 데이터에 포함되지 않은 test 데이터>\\nLoss: {bert_mismatched_result[0]:.3f}\\nAccuracy: {bert_mismatched_result[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abe8ecf7",
      "metadata": {
        "id": "abe8ecf7"
      },
      "source": [
        "### **5-2. RoBERTa**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2735175",
      "metadata": {
        "id": "b2735175",
        "outputId": "3fdd864e-9c16-4573-a9fc-7ca3c6e05b0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "613/613 [==============================] - 26s 42ms/step - loss: 1.0620 - acc: 0.6649\n",
            "616/616 [==============================] - 26s 42ms/step - loss: 1.0624 - acc: 0.6642\n"
          ]
        }
      ],
      "source": [
        "roberta_matched_result = roberta_model.evaluate(robert_m_test)\n",
        "roberta_mismatched_result = roberta_model.evaluate(robert_mism_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81899b2a",
      "metadata": {
        "id": "81899b2a",
        "outputId": "2adb90cb-1fcd-4024-83f3-ebd4208a5f1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<학습 데이터에 포함된 test 데이터>\n",
            "Loss: 1.062\n",
            "Accuracy: 0.665\n",
            "\n",
            "\n",
            "<학습 데이터에 포함되지 않은 test 데이터>\n",
            "Loss: 1.062\n",
            "Accuracy: 0.664\n"
          ]
        }
      ],
      "source": [
        "print(f\"<학습 데이터에 포함된 test 데이터>\\nLoss: {roberta_matched_result[0]:.3f}\\nAccuracy: {roberta_matched_result[1]:.3f}\\n\\n\")\n",
        "print(f\"<학습 데이터에 포함되지 않은 test 데이터>\\nLoss: {roberta_mismatched_result[0]:.3f}\\nAccuracy: {roberta_mismatched_result[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc9186a6",
      "metadata": {
        "id": "cc9186a6"
      },
      "source": [
        "### **5-3. 최종 평가**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a630d6b7",
      "metadata": {
        "id": "a630d6b7"
      },
      "source": [
        "||Validation 데이터|학습 데이터에 포함된 Test 데이터|학습 데이터에 포함되지 않은 Test 데이터|\n",
        "|:---:|:---:|:---:|:---:|\n",
        "||**Loss / Accuracy**|**Loss / Accuracy**|**Loss / Accuracy**|\n",
        "|**BERT**|1.1041 / 0.6016|1.117 / 0.599|1.108 / 0.596|\n",
        "|**RoBERTa**|1.0624 / 0.6645|1.062 / 0.665|1.062 / 0.664|"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3300cca",
      "metadata": {
        "id": "c3300cca"
      },
      "source": [
        "## **6. 회고**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13abc338",
      "metadata": {
        "id": "13abc338"
      },
      "source": [
        "### **6-1. 프로젝트 회고**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc891342",
      "metadata": {
        "id": "cc891342"
      },
      "source": [
        "이번 프로젝트는 16번 노드 내용을 LMS상에서 진행했을 떄 굉장히 많은 오류를 만났기 때문에 잘 진행할 수 있는지 의문이 들었다. 이번 프로젝트는 로컬에서 진행을 했는데, 버전 호환 문제사 발생할 것이라고 생각하고 먼저 다른 사람의 코드를 똑같이 진행해 보았다. 그랬는데 생각보다 오류 한번 없이 잘 진행되었다! 그래서 생각보다 굉장히 수월하게 진행할 수 있었다.  \n",
        "\n",
        "중간에 학습데이터를 생성하는 과정에서 오류는 발생하지 않았지만 자꾸 경고 문구를 출력하는데, 데이터 한줄을 생성할 때 마다 한번씩 출력되어서 경고 문구를 삭제하는 라이브러리를 불러와서 사용했다. 아무래도 프레임워크를 가져와서 사용하는 것이다 보니 버전 문제만 해결되면 라이브러리를 불러와서 이런 저런 수정을 편리하게 할 수 있는 부분이 아주 매력적이고, 잘 배워 놓으면 잘 사용할 수 있다는 생각이 들었다.  \n",
        "\n",
        "일단 다른 분의 코드를 똑같이 적는 방식으로 진행을 하였지만, 저번까지 내가 모델을 구성했던 것에 비해 Hugging Face에서 제공하는 모델을 가져다 썼기 때문에 더 간단하게 작업이 가능한 점이 좋았던 것 같다. 대부분의 프레임워크가 파이토치를 주로 지원하는 점 때문에 지금까지는 tensorflow를 주로 사용했지만 파이토치도 사용해야 할 필요성이 느껴졌다. 시간이 난다면 파이토치를 조금씩 공부를 해야할 것 같다.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "239a38a2",
      "metadata": {
        "id": "239a38a2"
      },
      "source": [
        "### **6-2. 참고 자료**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01fa06ed",
      "metadata": {
        "id": "01fa06ed"
      },
      "source": [
        "1. https://github.com/YAGI0423/aiffel_going_deeper_nlp/blob/main/going_deeper_16/GD16_v3_1.ipynb"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "name": "[GD-16]nlp.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "cc5a3982",
        "08636fbc",
        "77b2cd41",
        "2a2561fc",
        "f5f9c074",
        "aee63bf0",
        "2d353618",
        "fb94e664",
        "099786ca",
        "3fca0cef",
        "abe8ecf7",
        "cc9186a6",
        "13abc338",
        "239a38a2"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}